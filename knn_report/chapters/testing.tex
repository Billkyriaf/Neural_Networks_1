\subsection*{KNN}
\subsubsection{Implementation}

The KNN algorithm is based on the fact that similar objects are close to each other. The distance between two images
denotes the similarity between them. If two images have overlapping features, from the Euclidean distance formula, those 
features will cancel each other out and the final distance will be small.

\begin{equation}
    d = \sqrt{\sum_{i=1}^{n} (x_i - y_i)^2}
    \label{eq:euclidean}
\end{equation}

The actual implementation of the Euclidean distance in the code does not use the square root. This is because the square root
is a relatively expensive function to implement. Also the square root is a monotonic function. This means that the order of the
distances is preserved.

An other optimization of the current implementation is that the final distances array is not sorted. The
K nearest neighbors are acquired by finding the minimum value of the distances array. The time complexity of this algorithm is
O(kn) instead of O(nlogn).

The K factor is the number of neighbors that the algorithm will use to classify the image. The values of K that were used for 
testing in this implementation are 1 and 3. In general we want to use an odd number for K. This is because if we use an even
number for K we might end up with a tie. Finally, the MNIST dataset has overlapping classes. This means that the classes are
not linearly separable. For this reason larger numbers for K lead to very low accuracy.

\subsubsection{Results}

The table below shows the accuracy and the execution time of the KNN algorithm for different values of K. The accuracy is
the percentage of the correctly classified images. The execution time is the time it took to classify all the images in the
test dataset. The execution time is measured in seconds. 16 threads were used for the parallelization of the algorithm.

\begin{center}
\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        K & 1 & 3 & 5 & 7 \\
        \hline
        Accuracy (\%) & 96.91 & 97.05 & 96.88 & 96.94 \\
        \hline
        Time (s) & 39.63 & 41.92 & 44.67 & 46.52 \\
        \hline
    \end{tabular}
    \caption{KNN accuracy}
    \label{tab:knn}
\end{table}
\end{center}


\subsection*{K-means}
\subsubsection{Implementation}

The K-means algorithm is an optimization of the KNN algorithm. Instead of calculating the distance between the test image and all the
training images, the K-means algorithm calculates the distance between the test image and the centroids of the clusters. The centroids
are the mean of the images in the cluster.

In order to start the classification process, the algorithm needs to initialize the centroids. The centroids are initialized by calculating
the mean image of every class. The mean image is calculated by adding all the images of a class and dividing the result by the number of
images in that class. The mean image is then used as the centroid of the cluster. The initialization cost of the algorithm is substantially
higher than the KNN algorithm.

But up site is that once the centroids are initialized, the algorithm can classify the images in a very efficient way. The centroids can be
saved in a file and reused for future classifications. The classification process is the same as the KNN algorithm.

\subsubsection{Results}

The K-means algorithm does not perform very well. The reason for this is that digits of the same class can appear in drasticaly different
positions. This means that the centroids could easily be placed in the middle of another class, thus leading to a lot of misclassifications.

The table below shows the accuracy and the execution time of the K-means algorithm. The accuracy is the percentage of the correctly
classified images. The execution time is the time it took to classify all the images in the test dataset. The execution time is measured
in seconds. 16 threads were used for the parallelization of the algorithm.


\begin{center}
    \begin{table}[h]
        \centering
        \begin{tabular}{|c|c|c|c|c|}
            \hline
            Run & 1 & 2 & 3 & 4 \\
            \hline
            Accuracy (\%) & 73.27 & 73.27 & 73.27 & 73.27 \\
            \hline
            Time (ms) & 65.8 & 61.16 & 68.42 & 63.55 \\
            \hline
        \end{tabular}
        \caption{K-Means accuracy}
        \label{tab:kmeans}
    \end{table}
    \end{center}

Because the cluster means are initialized from all the data the results of each run are the same.

\subsection*{K-means with Advanced Clustering}
\subsubsection{Implementation}

